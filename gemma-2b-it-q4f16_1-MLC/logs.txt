/home/cfruan/.conda/envs/mlc-source-311/bin/python -m mlc_chat gen_config /models/gemma-2b-it --quantization q4f16_1 --conv-template gemma_instruction --output /tmp/tmp8p1wh14f --context-window-size 8192 --prefill-chunk-size 1024
[2024-02-21 21:09:03] INFO auto_config.py:115: [92mFound[0m model configuration: /models/gemma-2b-it/config.json
[2024-02-21 21:09:03] INFO auto_config.py:153: [92mFound[0m model type: [1mgemma[0m. Use `--model-type` to override.
[2024-02-21 21:09:03] INFO gemma_model.py:55: [1mcontext_window_size[0m not found in config.json. Falling back to [1mmax_position_embeddings[0m (8192)
[2024-02-21 21:09:03] INFO gemma_model.py:70: [1mprefill_chunk_size[0m defaults to [1mcontext_window_size[0m (8192)
[2024-02-21 21:09:03] INFO config.py:106: Overriding [1mcontext_window_size[0m from 8192 to 8192
[2024-02-21 21:09:03] INFO config.py:106: Overriding [1mprefill_chunk_size[0m from 8192 to 1024
[2024-02-21 21:09:03] INFO config.py:106: Overriding [1mmax_batch_size[0m from 1 to 80
[2024-02-21 21:09:03] INFO gen_config.py:121: [generation_config.json] Setting [1mbos_token_id[0m: 2
[2024-02-21 21:09:03] INFO gen_config.py:121: [generation_config.json] Setting [1meos_token_id[0m: 1
[2024-02-21 21:09:03] INFO gen_config.py:121: [generation_config.json] Setting [1mpad_token_id[0m: 0
[2024-02-21 21:09:03] INFO gen_config.py:133: [92mFound[0m tokenizer config: /models/gemma-2b-it/tokenizer.model. Copying to [1m/tmp/tmp8p1wh14f/tokenizer.model[0m
[2024-02-21 21:09:03] INFO gen_config.py:133: [92mFound[0m tokenizer config: /models/gemma-2b-it/tokenizer.json. Copying to [1m/tmp/tmp8p1wh14f/tokenizer.json[0m
[2024-02-21 21:09:03] INFO gen_config.py:135: [91mNot found[0m tokenizer config: /models/gemma-2b-it/vocab.json
[2024-02-21 21:09:03] INFO gen_config.py:135: [91mNot found[0m tokenizer config: /models/gemma-2b-it/merges.txt
[2024-02-21 21:09:03] INFO gen_config.py:135: [91mNot found[0m tokenizer config: /models/gemma-2b-it/added_tokens.json
[2024-02-21 21:09:03] INFO gen_config.py:133: [92mFound[0m tokenizer config: /models/gemma-2b-it/tokenizer_config.json. Copying to [1m/tmp/tmp8p1wh14f/tokenizer_config.json[0m
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mtemperature[0m: 0.7
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mpresence_penalty[0m: 0.0
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mfrequency_penalty[0m: 0.0
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mrepetition_penalty[0m: 1.0
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mtop_p[0m: 0.95
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mmean_gen_len[0m: 128
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mmax_gen_len[0m: 512
[2024-02-21 21:09:03] INFO gen_config.py:74: [System default] Setting [1mshift_fill_factor[0m: 0.3
[2024-02-21 21:09:03] INFO gen_config.py:186: Dumping configuration file to: [1m/tmp/tmp8p1wh14f/mlc-chat-config.json[0m
/home/cfruan/.conda/envs/mlc-source-311/bin/python -m mlc_chat convert_weight /models/gemma-2b-it --quantization q4f16_1 --source-format auto --output /tmp/tmp8p1wh14f
[2024-02-21 21:09:05] INFO auto_config.py:115: [92mFound[0m model configuration: /models/gemma-2b-it/config.json
[2024-02-21 21:09:06] INFO auto_device.py:76: [92mFound[0m device: cuda:0
[2024-02-21 21:09:06] INFO auto_device.py:76: [92mFound[0m device: cuda:1
[2024-02-21 21:09:07] INFO auto_device.py:85: [91mNot found[0m device: rocm:0
[2024-02-21 21:09:08] INFO auto_device.py:85: [91mNot found[0m device: metal:0
[2024-02-21 21:09:19] INFO auto_device.py:76: [92mFound[0m device: vulkan:0
[2024-02-21 21:09:19] INFO auto_device.py:76: [92mFound[0m device: vulkan:1
[2024-02-21 21:09:19] INFO auto_device.py:76: [92mFound[0m device: vulkan:2
[2024-02-21 21:09:20] INFO auto_device.py:85: [91mNot found[0m device: opencl:0
[2024-02-21 21:09:20] INFO auto_device.py:33: Using device: [1mcuda:0[0m
[2024-02-21 21:09:20] INFO auto_weight.py:70: Finding weights in: /models/gemma-2b-it
[2024-02-21 21:09:20] INFO auto_weight.py:136: [91mNot found[0m Huggingface PyTorch
[2024-02-21 21:09:20] INFO auto_weight.py:143: [92mFound[0m source weight format: huggingface-safetensor. Source configuration: /models/gemma-2b-it/model.safetensors.index.json
[2024-02-21 21:09:20] INFO auto_weight.py:106: Using source weight configuration: [1m/models/gemma-2b-it/model.safetensors.index.json[0m. Use `--source` to override.
[2024-02-21 21:09:20] INFO auto_weight.py:110: Using source weight format: [1mhuggingface-safetensor[0m. Use `--source-format` to override.
[2024-02-21 21:09:20] INFO auto_config.py:153: [92mFound[0m model type: [1mgemma[0m. Use `--model-type` to override.
[2024-02-21 21:09:20] INFO gemma_model.py:55: [1mcontext_window_size[0m not found in config.json. Falling back to [1mmax_position_embeddings[0m (8192)
[2024-02-21 21:09:20] INFO gemma_model.py:70: [1mprefill_chunk_size[0m defaults to [1mcontext_window_size[0m (8192)
[1mWeight conversion with arguments:[0m
  [1m--config[0m          /models/gemma-2b-it/config.json
  [1m--quantization[0m    GroupQuantize(name='q4f16_1', kind='group-quant', group_size=32, quantize_dtype='int4', storage_dtype='uint32', model_dtype='float16', linear_weight_layout='NK', quantize_embedding=True, quantize_final_fc=True, num_elem_per_storage=8, num_storage_per_group=4, max_int_value=7)
  [1m--model-type[0m      gemma
  [1m--device[0m          cuda:0
  [1m--source[0m          /models/gemma-2b-it/model.safetensors.index.json
  [1m--source-format[0m   huggingface-safetensor
  [1m--output[0m          /tmp/tmp8p1wh14f
  0%|                                                                                                                                                                                                                                                                  | 0/110 [00:00<?, ?it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:22] INFO huggingface_loader.py:182: Loading HF parameters from: /models/gemma-2b-it/model-00001-of-00002.safetensors
  0%|                                                                                                                                                                                                                                                                  | 0/110 [00:00<?, ?it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:38] INFO group_quantization.py:232: Compiling quantize function for key: ((256000, 2048), float16, cuda, axis=1, output_transpose=False)
  0%|                                                                                                                                                                                                                                                                  | 0/110 [00:15<?, ?it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:39] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.embed_tokens.q_weight[0m", shape: (256000, 256), dtype: uint32
  0%|                                                                                                                                                                                                                                                                  | 0/110 [00:17<?, ?it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:40] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.embed_tokens.q_scale[0m", shape: (256000, 64), dtype: float16
  0%|                                                                                                                                                                                                                                                                  | 0/110 [00:17<?, ?it/s]  1%|██▎                                                                                                                                                                                                                                                       | 1/110 [00:17<32:03, 17.65s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:40] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.0.input_layernorm.weight[0m", shape: (2048,), dtype: float16
  1%|██▎                                                                                                                                                                                                                                                       | 1/110 [00:17<32:03, 17.65s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:40] INFO group_quantization.py:232: Compiling quantize function for key: ((2048, 16384), float16, cuda, axis=1, output_transpose=False)
  1%|██▎                                                                                                                                                                                                                                                       | 1/110 [00:17<32:03, 17.65s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:40] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
  1%|██▎                                                                                                                                                                                                                                                       | 1/110 [00:18<32:03, 17.65s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:40] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
  1%|██▎                                                                                                                                                                                                                                                       | 1/110 [00:18<32:03, 17.65s/it]  3%|██████▊                                                                                                                                                                                                                                                   | 3/110 [00:18<08:34,  4.81s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:41] INFO group_quantization.py:232: Compiling quantize function for key: ((32768, 2048), float16, cuda, axis=1, output_transpose=False)
  3%|██████▊                                                                                                                                                                                                                                                   | 3/110 [00:19<08:34,  4.81s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
  3%|██████▊                                                                                                                                                                                                                                                   | 3/110 [00:19<08:34,  4.81s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
  3%|██████▊                                                                                                                                                                                                                                                   | 3/110 [00:19<08:34,  4.81s/it]  4%|█████████                                                                                                                                                                                                                                                 | 4/110 [00:19<06:28,  3.67s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.0.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
  4%|█████████                                                                                                                                                                                                                                                 | 4/110 [00:19<06:28,  3.67s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO group_quantization.py:232: Compiling quantize function for key: ((2560, 2048), float16, cuda, axis=1, output_transpose=False)
  4%|█████████                                                                                                                                                                                                                                                 | 4/110 [00:19<06:28,  3.67s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
  4%|█████████                                                                                                                                                                                                                                                 | 4/110 [00:20<06:28,  3.67s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
  4%|█████████                                                                                                                                                                                                                                                 | 4/110 [00:20<06:28,  3.67s/it]  5%|█████████████▋                                                                                                                                                                                                                                            | 6/110 [00:20<03:24,  1.97s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:42] INFO group_quantization.py:232: Compiling quantize function for key: ((2048, 2048), float16, cuda, axis=1, output_transpose=False)
  5%|█████████████▋                                                                                                                                                                                                                                            | 6/110 [00:20<03:24,  1.97s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:43] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
  5%|█████████████▋                                                                                                                                                                                                                                            | 6/110 [00:20<03:24,  1.97s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:43] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.0.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
  5%|█████████████▋                                                                                                                                                                                                                                            | 6/110 [00:20<03:24,  1.97s/it]  6%|███████████████▉                                                                                                                                                                                                                                          | 7/110 [00:20<02:41,  1.57s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:43] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.1.input_layernorm.weight[0m", shape: (2048,), dtype: float16
  6%|███████████████▉                                                                                                                                                                                                                                          | 7/110 [00:20<02:41,  1.57s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:43] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
  6%|███████████████▉                                                                                                                                                                                                                                          | 7/110 [00:20<02:41,  1.57s/it]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:43] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
  6%|███████████████▉                                                                                                                                                                                                                                          | 7/110 [00:20<02:41,  1.57s/it]  8%|████████████████████▍                                                                                                                                                                                                                                     | 9/110 [00:20<01:34,  1.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
  8%|████████████████████▍                                                                                                                                                                                                                                     | 9/110 [00:22<01:34,  1.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
  8%|████████████████████▍                                                                                                                                                                                                                                     | 9/110 [00:22<01:34,  1.06it/s]  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.1.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.1.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.10.input_layernorm.weight[0m", shape: (2048,), dtype: float16
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:44] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
  9%|██████████████████████▋                                                                                                                                                                                                                                  | 10/110 [00:22<01:38,  1.02it/s] 14%|█████████████████████████████████▉                                                                                                                                                                                                                       | 15/110 [00:22<00:38,  2.47it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 14%|█████████████████████████████████▉                                                                                                                                                                                                                       | 15/110 [00:23<00:38,  2.47it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 14%|█████████████████████████████████▉                                                                                                                                                                                                                       | 15/110 [00:23<00:38,  2.47it/s] 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.10.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.10.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:45] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.11.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:46] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:46] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 15%|████████████████████████████████████▏                                                                                                                                                                                                                    | 16/110 [00:23<00:46,  2.02it/s] 19%|███████████████████████████████████████████████▌                                                                                                                                                                                                         | 21/110 [00:23<00:24,  3.69it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:46] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 19%|███████████████████████████████████████████████▌                                                                                                                                                                                                         | 21/110 [00:24<00:24,  3.69it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:46] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 19%|███████████████████████████████████████████████▌                                                                                                                                                                                                         | 21/110 [00:24<00:24,  3.69it/s] 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:46] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.11.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.11.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.12.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:47] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 20%|█████████████████████████████████████████████████▊                                                                                                                                                                                                       | 22/110 [00:24<00:30,  2.85it/s] 25%|█████████████████████████████████████████████████████████████                                                                                                                                                                                            | 27/110 [00:24<00:17,  4.76it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 25%|█████████████████████████████████████████████████████████████                                                                                                                                                                                            | 27/110 [00:25<00:17,  4.76it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 25%|█████████████████████████████████████████████████████████████                                                                                                                                                                                            | 27/110 [00:25<00:17,  4.76it/s] 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.12.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.12.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.13.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:25<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:26<00:23,  3.46it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:48] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 25%|███████████████████████████████████████████████████████████████▍                                                                                                                                                                                         | 28/110 [00:26<00:23,  3.46it/s] 30%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                              | 33/110 [00:26<00:14,  5.41it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 30%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                              | 33/110 [00:26<00:14,  5.41it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 30%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                              | 33/110 [00:26<00:14,  5.41it/s] 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:26<00:20,  3.68it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.13.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:26<00:20,  3.68it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:27<00:20,  3.68it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:27<00:20,  3.68it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:27<00:20,  3.68it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.13.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 31%|████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                            | 34/110 [00:27<00:20,  3.68it/s] 34%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                     | 37/110 [00:27<00:14,  5.10it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.14.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 34%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                     | 37/110 [00:27<00:14,  5.10it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 34%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                     | 37/110 [00:27<00:14,  5.10it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:49] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 34%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                     | 37/110 [00:27<00:14,  5.10it/s] 35%|████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 39/110 [00:27<00:12,  5.60it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 35%|████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 39/110 [00:28<00:12,  5.60it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 35%|████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 39/110 [00:28<00:12,  5.60it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.14.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 35%|████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                | 39/110 [00:28<00:12,  5.60it/s] 37%|████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                            | 41/110 [00:28<00:16,  4.14it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 37%|████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                            | 41/110 [00:28<00:16,  4.14it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 37%|████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                            | 41/110 [00:28<00:16,  4.14it/s] 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.14.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.15.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:50] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 42/110 [00:28<00:14,  4.54it/s] 41%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                   | 45/110 [00:28<00:10,  5.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 41%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                   | 45/110 [00:29<00:10,  5.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 41%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                   | 45/110 [00:29<00:10,  5.96it/s] 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.15.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.15.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:51] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.16.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:52] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:52] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 46/110 [00:29<00:17,  3.56it/s] 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                     | 51/110 [00:29<00:09,  6.31it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                     | 51/110 [00:30<00:09,  6.31it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                     | 51/110 [00:30<00:09,  6.31it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.16.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 46%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                     | 51/110 [00:30<00:09,  6.31it/s] 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:30<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:30<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:30<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:30<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.16.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:30<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:31<00:13,  4.32it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:53] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 48%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                 | 53/110 [00:31<00:13,  4.32it/s] 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.2.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:54] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                          | 56/110 [00:31<00:13,  3.86it/s] 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 60/110 [00:31<00:09,  5.34it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 60/110 [00:32<00:09,  5.34it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                 | 60/110 [00:32<00:09,  5.34it/s] 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.2.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.2.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.3.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:55] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                               | 61/110 [00:32<00:12,  3.88it/s] 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                   | 66/110 [00:32<00:07,  6.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                   | 66/110 [00:33<00:07,  6.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                   | 66/110 [00:33<00:07,  6.06it/s] 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.3.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.3.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 67/110 [00:33<00:10,  4.11it/s] 64%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                          | 70/110 [00:33<00:06,  5.74it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.4.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 64%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                          | 70/110 [00:33<00:06,  5.74it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 64%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                          | 70/110 [00:34<00:06,  5.74it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:56] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 64%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                          | 70/110 [00:34<00:06,  5.74it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                      | 72/110 [00:34<00:06,  6.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                      | 72/110 [00:34<00:06,  6.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                      | 72/110 [00:34<00:06,  6.06it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.4.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                      | 72/110 [00:34<00:06,  6.06it/s] 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.4.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.5.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:34<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:35<00:08,  4.49it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:57] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 74/110 [00:35<00:08,  4.49it/s] 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 78/110 [00:35<00:05,  6.38it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 78/110 [00:36<00:05,  6.38it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 78/110 [00:36<00:05,  6.38it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.5.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 78/110 [00:36<00:05,  6.38it/s] 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 80/110 [00:36<00:07,  4.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 80/110 [00:36<00:07,  4.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 80/110 [00:36<00:07,  4.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 80/110 [00:36<00:07,  4.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.5.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 80/110 [00:36<00:07,  4.13it/s] 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 82/110 [00:36<00:05,  5.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:58] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.6.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 82/110 [00:36<00:05,  5.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:59] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 82/110 [00:36<00:05,  5.13it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:59] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 82/110 [00:36<00:05,  5.13it/s] 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 84/110 [00:36<00:04,  5.55it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:09:59] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 84/110 [00:37<00:04,  5.55it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 84/110 [00:37<00:04,  5.55it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.6.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 84/110 [00:37<00:04,  5.55it/s] 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.6.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.7.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:00] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 86/110 [00:37<00:06,  3.79it/s] 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 90/110 [00:37<00:03,  5.57it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 90/110 [00:38<00:03,  5.57it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                             | 90/110 [00:38<00:03,  5.57it/s] 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.7.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.7.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.8.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:38<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:39<00:05,  3.61it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:01] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 91/110 [00:39<00:05,  3.61it/s] 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 96/110 [00:39<00:02,  6.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 96/110 [00:40<00:02,  6.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 96/110 [00:40<00:02,  6.02it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.8.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 96/110 [00:40<00:02,  6.02it/s] 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:40<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.8.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.9.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:03] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 98/110 [00:41<00:04,  2.96it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 102/110 [00:41<00:01,  4.24it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.mlp.gate_up_proj.q_weight[0m", shape: (32768, 256), dtype: uint32
 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 102/110 [00:41<00:01,  4.24it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.mlp.gate_up_proj.q_scale[0m", shape: (32768, 64), dtype: float16
 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 102/110 [00:41<00:01,  4.24it/s] 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.9.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.self_attn.qkv_proj.q_weight[0m", shape: (2560, 256), dtype: uint32
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.self_attn.qkv_proj.q_scale[0m", shape: (2560, 64), dtype: float16
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.self_attn.o_proj.q_weight[0m", shape: (2048, 256), dtype: uint32
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.9.self_attn.o_proj.q_scale[0m", shape: (2048, 64), dtype: float16
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:04] INFO huggingface_loader.py:194: Unloading HF weight file: /models/gemma-2b-it/model-00001-of-00002.safetensors
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:41<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:05] INFO huggingface_loader.py:182: Loading HF parameters from: /models/gemma-2b-it/model-00002-of-00002.safetensors
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:42<00:01,  3.59it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:05] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.17.input_layernorm.weight[0m", shape: (2048,), dtype: float16
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 103/110 [00:42<00:01,  3.59it/s] 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 107/110 [00:42<00:00,  3.65it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:06] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.mlp.down_proj.q_weight[0m", shape: (2048, 2048), dtype: uint32
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 107/110 [00:43<00:00,  3.65it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:06] INFO huggingface_loader.py:164: [Quantized] Parameter: "[1mmodel.layers.17.mlp.down_proj.q_scale[0m", shape: (2048, 512), dtype: float16
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 107/110 [00:43<00:00,  3.65it/s] 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 108/110 [00:43<00:00,  2.70it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:06] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.layers.17.post_attention_layernorm.weight[0m", shape: (2048,), dtype: float16
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 108/110 [00:43<00:00,  2.70it/s]                                                                                                                                                                                                                                                                                               [2024-02-21 21:10:06] INFO huggingface_loader.py:172: [Not quantized] Parameter: "[1mmodel.norm.weight[0m", shape: (2048,), dtype: float16
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 108/110 [00:43<00:00,  2.70it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:43<00:00,  2.50it/s]
[2024-02-21 21:10:06] INFO huggingface_loader.py:194: Unloading HF weight file: /models/gemma-2b-it/model-00002-of-00002.safetensors
[2024-02-21 21:10:06] INFO stats.py:76: [92mTime usage[0m: HF loading: 12.610 sec; Pre-quantization mapping: 25.853 sec; Quantization: 3.268 sec
[2024-02-21 21:10:06] INFO stats.py:90: [92mRAM usage[0m: Peak RAM: 9.211 GB. Total bytes loaded from disk: 9.336 GB
[2024-02-21 21:10:06] INFO convert_weight.py:132: [92mParameter size[0m after quantization: 1.313 GB
[2024-02-21 21:10:06] INFO convert_weight.py:137: [92mTotal parameters[0m: 2,506,172,416
[2024-02-21 21:10:06] INFO convert_weight.py:138: [92mBits per parameter[0m: 4.500
Start storing to cache /tmp/tmp8p1wh14f
[0001/0183] saving model.embed_tokens.q_weight                                              [0002/0183] saving model.embed_tokens.q_scale                                              [0003/0183] saving model.layers.0.input_layernorm.weight                                                        [0004/0183] saving model.layers.0.mlp.down_proj.q_weight                                                        [0005/0183] saving model.layers.0.mlp.down_proj.q_scale                                                        [0006/0183] saving model.layers.0.mlp.gate_up_proj.q_weight                                                           [0007/0183] saving model.layers.0.mlp.gate_up_proj.q_scale                                                           [0008/0183] saving model.layers.0.post_attention_layernorm.weight                                                                 [0009/0183] saving model.layers.0.self_attn.qkv_proj.q_weight                                                                 [0010/0183] saving model.layers.0.self_attn.qkv_proj.q_scale                                                                 [0011/0183] saving model.layers.0.self_attn.o_proj.q_weight                                                                 [0012/0183] saving model.layers.0.self_attn.o_proj.q_scale                                                                 [0013/0183] saving model.layers.1.input_layernorm.weight                                                                 [0014/0183] saving model.layers.1.mlp.down_proj.q_weight                                                                 [0015/0183] saving model.layers.1.mlp.down_proj.q_scale                                                                 [0016/0183] saving model.layers.1.mlp.gate_up_proj.q_weight                                                                 [0017/0183] saving model.layers.1.mlp.gate_up_proj.q_scale                                                                 [0018/0183] saving model.layers.1.post_attention_layernorm.weight                                                                 [0019/0183] saving model.layers.1.self_attn.qkv_proj.q_weight                                                                 [0020/0183] saving model.layers.1.self_attn.qkv_proj.q_scale                                                                 [0021/0183] saving model.layers.1.self_attn.o_proj.q_weight                                                                 [0022/0183] saving model.layers.1.self_attn.o_proj.q_scale                                                                 [0023/0183] saving model.layers.10.input_layernorm.weight                                                                 [0024/0183] saving model.layers.10.mlp.down_proj.q_weight                                                                 [0025/0183] saving model.layers.10.mlp.down_proj.q_scale                                                                 [0026/0183] saving model.layers.10.mlp.gate_up_proj.q_weight                                                                 [0027/0183] saving model.layers.10.mlp.gate_up_proj.q_scale                                                                 [0028/0183] saving model.layers.10.post_attention_layernorm.weight                                                                  [0029/0183] saving model.layers.10.self_attn.qkv_proj.q_weight                                                                  [0030/0183] saving model.layers.10.self_attn.qkv_proj.q_scale                                                                  [0031/0183] saving model.layers.10.self_attn.o_proj.q_weight                                                                  [0032/0183] saving model.layers.10.self_attn.o_proj.q_scale                                                                  [0033/0183] saving model.layers.11.input_layernorm.weight                                                                  [0034/0183] saving model.layers.11.mlp.down_proj.q_weight                                                                  [0035/0183] saving model.layers.11.mlp.down_proj.q_scale                                                                  [0036/0183] saving model.layers.11.mlp.gate_up_proj.q_weight                                                                  [0037/0183] saving model.layers.11.mlp.gate_up_proj.q_scale                                                                  [0038/0183] saving model.layers.11.post_attention_layernorm.weight                                                                  [0039/0183] saving model.layers.11.self_attn.qkv_proj.q_weight                                                                  [0040/0183] saving model.layers.11.self_attn.qkv_proj.q_scale                                                                  [0041/0183] saving model.layers.11.self_attn.o_proj.q_weight                                                                  [0042/0183] saving model.layers.11.self_attn.o_proj.q_scale                                                                  [0043/0183] saving model.layers.12.input_layernorm.weight                                                                  [0044/0183] saving model.layers.12.mlp.down_proj.q_weight                                                                  [0045/0183] saving model.layers.12.mlp.down_proj.q_scale                                                                  [0046/0183] saving model.layers.12.mlp.gate_up_proj.q_weight                                                                  [0047/0183] saving model.layers.12.mlp.gate_up_proj.q_scale                                                                  [0048/0183] saving model.layers.12.post_attention_layernorm.weight                                                                  [0049/0183] saving model.layers.12.self_attn.qkv_proj.q_weight                                                                  [0050/0183] saving model.layers.12.self_attn.qkv_proj.q_scale                                                                  [0051/0183] saving model.layers.12.self_attn.o_proj.q_weight                                                                  [0052/0183] saving model.layers.12.self_attn.o_proj.q_scale                                                                  [0053/0183] saving model.layers.13.input_layernorm.weight                                                                  [0054/0183] saving model.layers.13.mlp.down_proj.q_weight                                                                  [0055/0183] saving model.layers.13.mlp.down_proj.q_scale                                                                  [0056/0183] saving model.layers.13.mlp.gate_up_proj.q_weight                                                                  [0057/0183] saving model.layers.13.mlp.gate_up_proj.q_scale                                                                  [0058/0183] saving model.layers.13.post_attention_layernorm.weight                                                                  [0059/0183] saving model.layers.13.self_attn.qkv_proj.q_weight                                                                  [0060/0183] saving model.layers.13.self_attn.qkv_proj.q_scale                                                                  [0061/0183] saving model.layers.13.self_attn.o_proj.q_weight                                                                  [0062/0183] saving model.layers.13.self_attn.o_proj.q_scale                                                                  [0063/0183] saving model.layers.14.input_layernorm.weight                                                                  [0064/0183] saving model.layers.14.mlp.down_proj.q_weight                                                                  [0065/0183] saving model.layers.14.mlp.down_proj.q_scale                                                                  [0066/0183] saving model.layers.14.mlp.gate_up_proj.q_weight                                                                  [0067/0183] saving model.layers.14.mlp.gate_up_proj.q_scale                                                                  [0068/0183] saving model.layers.14.post_attention_layernorm.weight                                                                  [0069/0183] saving model.layers.14.self_attn.qkv_proj.q_weight                                                                  [0070/0183] saving model.layers.14.self_attn.qkv_proj.q_scale                                                                  [0071/0183] saving model.layers.14.self_attn.o_proj.q_weight                                                                  [0072/0183] saving model.layers.14.self_attn.o_proj.q_scale                                                                  [0073/0183] saving model.layers.15.input_layernorm.weight                                                                  [0074/0183] saving model.layers.15.mlp.down_proj.q_weight                                                                  [0075/0183] saving model.layers.15.mlp.down_proj.q_scale                                                                  [0076/0183] saving model.layers.15.mlp.gate_up_proj.q_weight                                                                  [0077/0183] saving model.layers.15.mlp.gate_up_proj.q_scale                                                                  [0078/0183] saving model.layers.15.post_attention_layernorm.weight                                                                  [0079/0183] saving model.layers.15.self_attn.qkv_proj.q_weight                                                                  [0080/0183] saving model.layers.15.self_attn.qkv_proj.q_scale                                                                  [0081/0183] saving model.layers.15.self_attn.o_proj.q_weight                                                                  [0082/0183] saving model.layers.15.self_attn.o_proj.q_scale                                                                  [0083/0183] saving model.layers.16.input_layernorm.weight                                                                  [0084/0183] saving model.layers.16.mlp.down_proj.q_weight                                                                  [0085/0183] saving model.layers.16.mlp.down_proj.q_scale                                                                  [0086/0183] saving model.layers.16.mlp.gate_up_proj.q_weight                                                                  [0087/0183] saving model.layers.16.mlp.gate_up_proj.q_scale                                                                  [0088/0183] saving model.layers.16.post_attention_layernorm.weight                                                                  [0089/0183] saving model.layers.16.self_attn.qkv_proj.q_weight                                                                  [0090/0183] saving model.layers.16.self_attn.qkv_proj.q_scale                                                                  [0091/0183] saving model.layers.16.self_attn.o_proj.q_weight                                                                  [0092/0183] saving model.layers.16.self_attn.o_proj.q_scale                                                                  [0093/0183] saving model.layers.17.mlp.gate_up_proj.q_weight                                                                  [0094/0183] saving model.layers.17.mlp.gate_up_proj.q_scale                                                                  [0095/0183] saving model.layers.17.self_attn.qkv_proj.q_weight                                                                  [0096/0183] saving model.layers.17.self_attn.qkv_proj.q_scale                                                                  [0097/0183] saving model.layers.17.self_attn.o_proj.q_weight                                                                  [0098/0183] saving model.layers.17.self_attn.o_proj.q_scale                                                                  [0099/0183] saving model.layers.2.input_layernorm.weight                                                                  [0100/0183] saving model.layers.2.mlp.down_proj.q_weight                                                                  [0101/0183] saving model.layers.2.mlp.down_proj.q_scale                                                                  [0102/0183] saving model.layers.2.mlp.gate_up_proj.q_weight                                                                  [0103/0183] saving model.layers.2.mlp.gate_up_proj.q_scale                                                                  [0104/0183] saving model.layers.2.post_attention_layernorm.weight                                                                  [0105/0183] saving model.layers.2.self_attn.qkv_proj.q_weight                                                                  [0106/0183] saving model.layers.2.self_attn.qkv_proj.q_scale                                                                  [0107/0183] saving model.layers.2.self_attn.o_proj.q_weight                                                                  [0108/0183] saving model.layers.2.self_attn.o_proj.q_scale                                                                  [0109/0183] saving model.layers.3.input_layernorm.weight                                                                  [0110/0183] saving model.layers.3.mlp.down_proj.q_weight                                                                  [0111/0183] saving model.layers.3.mlp.down_proj.q_scale                                                                  [0112/0183] saving model.layers.3.mlp.gate_up_proj.q_weight                                                                  [0113/0183] saving model.layers.3.mlp.gate_up_proj.q_scale                                                                  [0114/0183] saving model.layers.3.post_attention_layernorm.weight                                                                  [0115/0183] saving model.layers.3.self_attn.qkv_proj.q_weight                                                                  [0116/0183] saving model.layers.3.self_attn.qkv_proj.q_scale                                                                  [0117/0183] saving model.layers.3.self_attn.o_proj.q_weight                                                                  [0118/0183] saving model.layers.3.self_attn.o_proj.q_scale                                                                  [0119/0183] saving model.layers.4.input_layernorm.weight                                                                  [0120/0183] saving model.layers.4.mlp.down_proj.q_weight                                                                  [0121/0183] saving model.layers.4.mlp.down_proj.q_scale                                                                  [0122/0183] saving model.layers.4.mlp.gate_up_proj.q_weight                                                                  [0123/0183] saving model.layers.4.mlp.gate_up_proj.q_scale                                                                  [0124/0183] saving model.layers.4.post_attention_layernorm.weight                                                                  [0125/0183] saving model.layers.4.self_attn.qkv_proj.q_weight                                                                  [0126/0183] saving model.layers.4.self_attn.qkv_proj.q_scale                                                                  [0127/0183] saving model.layers.4.self_attn.o_proj.q_weight                                                                  [0128/0183] saving model.layers.4.self_attn.o_proj.q_scale                                                                  [0129/0183] saving model.layers.5.input_layernorm.weight[2024-02-21 21:10:15] INFO convert_weight.py:154: Saved to directory: [1m/tmp/tmp8p1wh14f[0m
                                                                  [0130/0183] saving model.layers.5.mlp.down_proj.q_weight                                                                  [0131/0183] saving model.layers.5.mlp.down_proj.q_scale                                                                  [0132/0183] saving model.layers.5.mlp.gate_up_proj.q_weight                                                                  [0133/0183] saving model.layers.5.mlp.gate_up_proj.q_scale                                                                  [0134/0183] saving model.layers.5.post_attention_layernorm.weight                                                                  [0135/0183] saving model.layers.5.self_attn.qkv_proj.q_weight                                                                  [0136/0183] saving model.layers.5.self_attn.qkv_proj.q_scale                                                                  [0137/0183] saving model.layers.5.self_attn.o_proj.q_weight                                                                  [0138/0183] saving model.layers.5.self_attn.o_proj.q_scale                                                                  [0139/0183] saving model.layers.6.input_layernorm.weight                                                                  [0140/0183] saving model.layers.6.mlp.down_proj.q_weight                                                                  [0141/0183] saving model.layers.6.mlp.down_proj.q_scale                                                                  [0142/0183] saving model.layers.6.mlp.gate_up_proj.q_weight                                                                  [0143/0183] saving model.layers.6.mlp.gate_up_proj.q_scale                                                                  [0144/0183] saving model.layers.6.post_attention_layernorm.weight                                                                  [0145/0183] saving model.layers.6.self_attn.qkv_proj.q_weight                                                                  [0146/0183] saving model.layers.6.self_attn.qkv_proj.q_scale                                                                  [0147/0183] saving model.layers.6.self_attn.o_proj.q_weight                                                                  [0148/0183] saving model.layers.6.self_attn.o_proj.q_scale                                                                  [0149/0183] saving model.layers.7.input_layernorm.weight                                                                  [0150/0183] saving model.layers.7.mlp.down_proj.q_weight                                                                  [0151/0183] saving model.layers.7.mlp.down_proj.q_scale                                                                  [0152/0183] saving model.layers.7.mlp.gate_up_proj.q_weight                                                                  [0153/0183] saving model.layers.7.mlp.gate_up_proj.q_scale                                                                  [0154/0183] saving model.layers.7.post_attention_layernorm.weight                                                                  [0155/0183] saving model.layers.7.self_attn.qkv_proj.q_weight                                                                  [0156/0183] saving model.layers.7.self_attn.qkv_proj.q_scale                                                                  [0157/0183] saving model.layers.7.self_attn.o_proj.q_weight                                                                  [0158/0183] saving model.layers.7.self_attn.o_proj.q_scale                                                                  [0159/0183] saving model.layers.8.input_layernorm.weight                                                                  [0160/0183] saving model.layers.8.mlp.down_proj.q_weight                                                                  [0161/0183] saving model.layers.8.mlp.down_proj.q_scale                                                                  [0162/0183] saving model.layers.8.mlp.gate_up_proj.q_weight                                                                  [0163/0183] saving model.layers.8.mlp.gate_up_proj.q_scale                                                                  [0164/0183] saving model.layers.8.post_attention_layernorm.weight                                                                  [0165/0183] saving model.layers.8.self_attn.qkv_proj.q_weight                                                                  [0166/0183] saving model.layers.8.self_attn.qkv_proj.q_scale                                                                  [0167/0183] saving model.layers.8.self_attn.o_proj.q_weight                                                                  [0168/0183] saving model.layers.8.self_attn.o_proj.q_scale                                                                  [0169/0183] saving model.layers.9.input_layernorm.weight                                                                  [0170/0183] saving model.layers.9.mlp.down_proj.q_weight                                                                  [0171/0183] saving model.layers.9.mlp.down_proj.q_scale                                                                  [0172/0183] saving model.layers.9.mlp.gate_up_proj.q_weight                                                                  [0173/0183] saving model.layers.9.mlp.gate_up_proj.q_scale                                                                  [0174/0183] saving model.layers.9.post_attention_layernorm.weight                                                                  [0175/0183] saving model.layers.9.self_attn.qkv_proj.q_weight                                                                  [0176/0183] saving model.layers.9.self_attn.qkv_proj.q_scale                                                                  [0177/0183] saving model.layers.9.self_attn.o_proj.q_weight                                                                  [0178/0183] saving model.layers.9.self_attn.o_proj.q_scale                                                                  [0179/0183] saving model.layers.17.input_layernorm.weight                                                                  [0180/0183] saving model.layers.17.mlp.down_proj.q_weight                                                                  [0181/0183] saving model.layers.17.mlp.down_proj.q_scale                                                                  [0182/0183] saving model.layers.17.post_attention_layernorm.weight                                                                  [0183/0183] saving model.norm.weight
All finished, 38 total shards committed, record saved to /tmp/tmp8p1wh14f/ndarray-cache.json
